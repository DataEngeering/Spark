import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
import org.apache.log4j.Logger
import org.apache.log4j.Level



object rddParllalize extends App{
  val conf = new SparkConf()
  conf.set("spark.app.name","RDD Parallalize")
  conf.set("spark.master","local[*]")
  
  val spark = SparkSession.builder().config(conf).getOrCreate()
  Logger.getLogger("org").setLevel(Level.ERROR)
  
  val logger = Logger.getLogger(getClass.getName)
  
  val sc = spark.sparkContext
  
  val list =List(1,2,3,4)
  
  val rdd =sc.parallelize(list)
  rdd.collect().foreach(println)
  
 logger.info(s"no of partition ${rdd.getNumPartitions}")
 
 logger.info(s"first value ${rdd.first()}")
 logger.info("defaultParallalism" + sc.defaultParallelism)
 
 logger.info("defaul min parallalism" + sc.defaultMinPartitions)
 
 val empty_rdd = sc.parallelize(Seq.empty[String])
}

Read Files:

  val baseRDD = sc.textFile("D:\\shiva\\POC\\SoumyaGit\\testdata\\ReadFileData\\*");
  
  val multiFileRDD = sc.textFile("D:\\shiva\\POC\\SoumyaGit\\testdata\\ReadFileData\\invalid.txt,D:\\shiva\\POC\\SoumyaGit\\testdata\\ReadFileData\\text01.txt");

  var wholeTextRDD = sc.wholeTextFiles("D:\\shiva\\POC\\SoumyaGit\\testdata\\ReadFileData\\*")

  var wholeTextRDD1 = sc.wholeTextFiles("D:\\shiva\\POC\\SoumyaGit\\testdata\\ReadFileData\\invalid.txt,D:\\shiva\\POC\\SoumyaGit\\testdata\\ReadFileData\\text01.txt")

  CSV:
  
 Skip Header From CSV file====================
When you have a header with column names in a CSV file and to read and process with Spark RDD, you need to skip the header as there is no way in RDD to specify your file has a header.

  var csvRdd = sc.textFile("D:\\Soumya\\git\\SparkPractice\\testdata\\ReadCSV\\*")
  val newRDD = csvRdd.mapPartitionsWithIndex((idx,iter) => if(idx == 0){iter.drop(1)} else iter)